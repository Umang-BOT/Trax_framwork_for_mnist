{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvLbsIQe_Yik"
   },
   "source": [
    "# Installation\n",
    "### To install Trax, you can use pip, the Python package installer. Open a terminal or command prompt and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JACTq0b92PW",
    "outputId": "4f953e18-557f-4915-aab9-73ff4cb2a7e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# !pip install trax\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CpDHhI_yGQ"
   },
   "source": [
    "### Quick Start\n",
    "#### Once Trax is installed, let's dive into a quick example to understand the basic workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOgdRUkA93tb",
    "outputId": "90a6b12b-9f48-4910-d9d1-862d6ee7a842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import trax.layers.base as tl_base\n",
    "import jax.numpy as np\n",
    "\n",
    "# Define your custom function\n",
    "def multiply_by_scalar(x, scalar):\n",
    "    return x * scalar\n",
    "\n",
    "# Create a function layer\n",
    "multiply_layer = tl_base.Fn('MultiplyByScalar', multiply_by_scalar)\n",
    "\n",
    "# Define some inputs\n",
    "x = 3\n",
    "scalar = 2\n",
    "\n",
    "# Apply the layer to the inputs\n",
    "result = multiply_layer((x, scalar))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWTh6DCV_8gK"
   },
   "source": [
    "In this example, we define a simple addition function and convert it into a Trax layer using trax.layers.Fn. We then apply the layer to inputs (a, b) and obtain the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC23v28yACOa"
   },
   "source": [
    "### Building Models\n",
    "#### Trax allows you to build complex models by stacking layers together. Here's an example of building a simple feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWI9SVs5-2Zd",
    "outputId": "55da5ac1-8dcf-4aef-8667-a3b69acada88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "import jax\n",
    "\n",
    "# Define the model architecture\n",
    "model = trax.layers.Serial(\n",
    "    trax.layers.Dense(128),\n",
    "    trax.layers.Relu(),\n",
    "    trax.layers.Dense(10)\n",
    ")\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = trax.shapes.ShapeDtype((1, 784), dtype='float32')\n",
    "\n",
    "# Initialize the model with the input shape\n",
    "model.init(input_shape)\n",
    "\n",
    "# Generate random input\n",
    "rng_key = jax.random.PRNGKey(0)  # Create a random number generator key\n",
    "inputs = trax.fastmath.random.normal(key=rng_key, shape=(1, 784))\n",
    "\n",
    "# Run the input through the model\n",
    "output = model(inputs)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)  # Output: (1, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxEQ_3URAIkP"
   },
   "source": [
    "In this example, we use trax.models.mlp.MLP to define a multi-layer perceptron (MLP) with a hidden layer of 128 units and an output layer of 10 units. We initialize the model with the input shape (1, 784), generate random inputs, and obtain the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6-Xe5sKANTW"
   },
   "source": [
    "### Training a Model\n",
    "#### Trax provides utilities for training models. Here's an example of training a simple model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNo-9R7oAUSv"
   },
   "outputs": [],
   "source": [
    "import trax\n",
    "import numpy as np\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "\n",
    "# Load the MNIST dataset and normalize the pixel values\n",
    "train_stream = trax.data.TFDS('mnist', keys=('image', 'label'), train=True)()\n",
    "eval_stream = trax.data.TFDS('mnist', keys=('image', 'label'), train=False)()\n",
    "\n",
    "def normalize_image(x):\n",
    "    image = x['image'].astype(np.float32) / 255.0\n",
    "    return image, x['label']  # Return the normalized image and the label\n",
    "\n",
    "train_stream = list(train_stream)\n",
    "eval_stream = list(eval_stream)\n",
    "\n",
    "# Apply normalization to the dataset\n",
    "train_stream = trax.data.Dataset(train_stream).map(normalize_image)\n",
    "eval_stream = trax.data.Dataset(eval_stream).map(normalize_image)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tl.Serial(\n",
    "    tl.Flatten(),  # Flatten the input (28, 28, 1) to (784,)\n",
    "    tl.Dense(256),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(10),\n",
    "    tl.LogSoftmax()\n",
    ")\n",
    "\n",
    "# Specify the input signature\n",
    "input_signature = trax.shapes.ShapeDtype((1, 28, 28, 1), dtype=np.float32)\n",
    "\n",
    "# Set the input signature for the model\n",
    "model.init(input_signature)\n",
    "\n",
    "# Define the custom loss function\n",
    "def custom_loss_fn():\n",
    "    def loss_fn(inputs, targets):\n",
    "        logits = model(inputs)\n",
    "        return tl.CrossEntropyLoss()(logits, targets)\n",
    "    return tl.Fn('CustomLoss', loss_fn)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = trax.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Define the training task\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_stream,\n",
    "    loss_layer=custom_loss_fn(),\n",
    "    optimizer=optimizer,\n",
    "    n_steps_per_checkpoint=500\n",
    ")\n",
    "\n",
    "# Define the evaluation task\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=eval_stream,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    ")\n",
    "\n",
    "# Create the training loop\n",
    "training_loop = training.Loop(\n",
    "    model=model,\n",
    "    tasks=[train_task],\n",
    "    eval_tasks=[eval_task]\n",
    ")\n",
    "\n",
    "# Run the training loop\n",
    "training_loop.run(n_steps=1000)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the MNIST dataset using trax.data.TFDS. We define a simple model using tl.Serial and specify the layers and their configuration. We define the loss function, optimizer, and create a TrainTask and EvalTask for training and evaluation, respectively. Finally, we create a training.Loop and run the training loop for a specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
